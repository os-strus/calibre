#!/usr/bin/env python
import re
import json
import time
from datetime import datetime, timedelta
from calibre.web.feeds.news import BasicNewsRecipe


def extract_json(raw):
    pre = re.search(r'<script>window.__preloadedData = ({.+)', raw).group(1)
    js = json.JSONDecoder().raw_decode(re.sub('undefined', '[]', pre))[0]
    return js['initialData']['data']['article']['sprinkledBody']['content']

def parse_image(i):
    if i['__typename'] == 'Image':
        yield '<div>'
        yield '<img src="{}">'.format(i['crops'][0]['renditions'][0]['url'])
        if i.get('caption'):
            yield '<div class="cap">{}'.format(
                i['caption'].get('text', '')
            )
            if i.get('credit'):
                yield '<span class="cred"> ' + i['credit'] + '</span>'
            yield '</div>'
        yield '</div>'

def parse_img_grid(g):
    for grd in g.get('gridMedia', {}):
        yield ''.join(parse_image(grd))
    if g.get('caption'):
        yield '<div class="cap">{}'.format(g['caption'])
        if g.get('credit'):
            yield '<span class="cred"> ' + g['credit'] + '</span>'
        yield '</div>'

def parse_cnt(cnt):
    txt = ''
    if cnt['__typename'] == 'TextInline':
        if cnt.get('formats'):
            for fmt in cnt.get('formats', {}):
                if fmt['__typename'] == 'ItalicFormat':
                    txt += '<i>'
                if fmt['__typename'] == 'LinkFormat':
                    txt += '<a href="{}">'.format(fmt['url'])
        txt += cnt['text']
    elif cnt['__typename'] == 'LineBreakInline':
        txt += '<br/>'
    if '<i>' in txt and '<a href' in txt:
        yield txt + '</a></i>'
    elif '<i>' in txt:
        yield txt + '</i>'
    elif '<a href' in txt:
        yield txt + '</a>'
    else:
        yield txt

def parse_byline(byl):
    for b in byl.get('bylines', {}):
        yield '<div>' + b['renderedRepresentation'] + '</div>'
    for rl in byl.get('role', {}):
        yield '<div><i>' + ''.join(parse_cnt(rl)) + '</i></div>'

def iso_date(x):
    dt = datetime.fromisoformat(x[:-1]) + timedelta(seconds=time.timezone)
    return dt.strftime('%b %d, %Y at %I:%M %p')

def header_parse(h):
    if h.get('label'):
        if h['label'].get('content'):
            for cl in h['label']['content']:
                yield '<div class="lbl">' + ''.join(parse_cnt(cl)) + '</div>'
    for ch in h['headline']['content']:
        yield '<h1>' + ''.join(parse_cnt(ch)) + '</h1>'
    if h.get('summary'):
        for cs in h['summary']['content']:
            yield '<p class="sub">' +  ''.join(parse_cnt(cs)) + '</p>'
    if h.get('ledeMedia'):
        if h['ledeMedia'].get('__typename', '') == 'ImageBlock':
            yield ''.join(parse_image(h['ledeMedia']['media']))
    if h.get('byline'):
        yield '<div class="byl"><br/>'
        yield '\t' + '\t'.join(parse_byline(h['byline']))
        if h.get('timestampBlock'):
            yield '\t<div>' + iso_date(h['timestampBlock']['timestamp']) + '</div>'
        yield '</div>'

def article_parse(data):
    yield "<html><body>"
    for x in data:
        if x.get('__typename', '') in {'HeaderBasicBlock', 'HeaderFullBleedVerticalBlock', 'HeaderFullBleedHorizontalBlock'}:
            yield '\n'.join(header_parse(x))
        elif x.get('__typename', '') == 'ParagraphBlock':
            p_txt = ''
            for para in x['content']:
                p_txt += ''.join(parse_cnt(para))
            if p_txt.strip():
                yield '<p>' + p_txt + '</p>'
        elif x.get('__typename', '') in {'Heading2Block', 'Heading3Block'}:
            h4_txt = ''
            for h2 in x['content']:
                h4_txt += ''.join(parse_cnt(h2))
            if h4_txt.strip():
                yield '<h4>' + h4_txt + '</h4>'
        elif x.get('__typename', '') == 'Heading1Block':
            h1_txt = ''
            for h1 in x['content']:
                h1_txt += ''.join(parse_cnt(h1))
            if h1_txt.strip():
                yield '<h1>' + h1_txt + '</h1>'
        elif x.get('__typename', '') == 'BylineBlock':
            yield '<div class="byl">\n<br/>\t' + '\t'.join(parse_byline(x)) + '</div>'
        elif x.get('__typename', '') == 'ImageBlock':
            yield ''.join(parse_image(x['media']))
        elif x.get('__typename', '') == 'GridBlock':
            yield ''.join(parse_img_grid(x))
        elif x.get('content'):
            o_txt = ''
            for i in x['content']:
                o_txt += ''.join(parse_cnt(i))
            if o_txt.strip():
                yield '<p><i>' + o_txt + '</i></p>'
    yield "</body></html>"


class nytFeeds(BasicNewsRecipe):
    title = 'NYT News'
    __author__ = 'unkn0wn'
    description = (
        'The New York Times is dedicated to helping people understand the world through '
        'on-the-ground, expert and deeply reported independent journalism. Feeds based recipe.'
    )
    oldest_article = 1
    encoding = 'utf-8'
    use_embedded_content = False
    language = 'en_US'
    remove_empty_feeds = True
    resolve_internal_links = True
    ignore_duplicate_articles = {'title', 'url'}
    masthead_url = 'https://static01.nytimes.com/newsgraphics/2015-12-23-masthead-2016/b15c3d81d3d7b59065fff9a3f3afe85aa2e2dff5/_assets/nyt-logo.png'

    def get_cover_url(self):
        soup = self.index_to_soup('https://www.frontpages.com/the-new-york-times/')
        return 'https://www.frontpages.com' + soup.find('img', attrs={'id':'giornale-img'})['src']

    recipe_specific_options = {
        'days': {
            'short': 'Oldest article to download from this news source. In days ',
            'long': 'For example, 0.5, gives you articles from the past 12 hours',
            'default': str(oldest_article)
        },
        'comp': {
            'short': 'Compress News Images?',
            'long': 'enter yes',
            'default': 'no'
        },
        'rev': {
            'short': 'Reverse the order of articles in each feed?',
            'long': 'enter yes',
            'default': 'no'
        },
        'res': {
            'short': 'For hi-res images, select a resolution from the\nfollowing options: popup, jumbo, mobileMasterAt3x, superJumbo',
            'long': 'This is useful for non e-ink devices, and for a lower file size\nthan the default(articleLarge), use articleInline.',
        }
    }

    def __init__(self, *args, **kwargs):
        BasicNewsRecipe.__init__(self, *args, **kwargs)
        d = self.recipe_specific_options.get('days')
        if d and isinstance(d, str):
            self.oldest_article = float(d)
        r = self.recipe_specific_options.get('rev')
        if r and isinstance(r, str):
            if r.lower() == 'yes':
                self.reverse_article_order = True
        c = self.recipe_specific_options.get('comp')
        if c and isinstance(c, str):
            if c.lower() == 'yes':
                self.compress_news_images = True

    extra_css = '''
        .byl { font-size:small; color:#202020; }
        .cap { font-size:small; text-align:center; }
        .cred { font-style:italic; font-size:small; }
        .sub { font-style:italic; }
        .lbl { font-size:small; color:#404040; }
        img { display:block; margin:0 auto; }
    '''

    feeds = [
        ('World', 'https://rss.nytimes.com/services/xml/rss/nyt/World.xml'),
        ('US', 'https://rss.nytimes.com/services/xml/rss/nyt/US.xml'),
        ('Business', 'https://rss.nytimes.com/services/xml/rss/nyt/Business.xml'),
        ('Technology', 'https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml'),
        ('Science', 'https://rss.nytimes.com/services/xml/rss/nyt/Science.xml'),
        ('Arts', 'https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml'),
        ('Fashion & Style', 'https://rss.nytimes.com/services/xml/rss/nyt/FashionandStyle.xml'),
        ('TMagazine', 'https://rss.nytimes.com/services/xml/rss/nyt/tmagazine.xml'),
        ('Travel', 'https://www.nytimes.com/services/xml/rss/nyt/Travel.xml'),
        ('Sunday Review', 'https://rss.nytimes.com/services/xml/rss/nyt/sunday-review.xml'),
    ]

    def get_browser(self, *args, **kwargs):
        kwargs['user_agent'] = 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'
        br = BasicNewsRecipe.get_browser(self, *args, **kwargs)
        br.addheaders += [
            ('Referer', 'https://www.google.com/'),
            ('X-Forwarded-For', '66.249.66.1')
        ]
        return br

    def preprocess_raw_html(self, raw_html, url):
        data = extract_json(raw_html)
        return '\n'.join(article_parse(data))

    def preprocess_html(self, soup):
        w = self.recipe_specific_options.get('res')
        if w and isinstance(w, str):
            res = '-' + w + '.jpg'
            for img in soup.findAll('img', attrs={'src':True}):
                img['src'] = img['src'].rsplit('-article', 1)[0] + res
        return soup
